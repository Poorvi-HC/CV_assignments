{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from dataset.st import SceneTextDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import detection\n",
    "from detection.faster_rcnn import FastRCNNPredictor\n",
    "from dataset.st import SceneTextDataset\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "from detection.image_list import ImageList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'background', 1: 'text'}\n",
      "Dataset size: 1003 samples\n",
      "Label mapping: {0: 'background', 1: 'text'}\n",
      "{'bboxes': tensor([[262.0000, 363.0000, 345.0000, 388.0000],\n",
      "        [275.0000, 394.0000, 333.0000, 419.0000],\n",
      "        [276.0826, 598.5223, 440.5027, 676.4273],\n",
      "        [441.4814, 592.0452, 872.1489, 678.2363],\n",
      "        [245.5036, 728.4973, 503.5211, 812.3753],\n",
      "        [661.8611, 740.7100, 919.8947, 823.8577],\n",
      "        [512.7561, 308.7530, 581.8213, 364.5263]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1])} ./Q1/img/img742.jpg\n",
      "Sample 0 - img742.jpg:\n",
      "./Q1/annots/img742.jpg.json\n",
      "Number of objects: 7\n",
      "  Object 0: {'xc': 303.4999694824219, 'yc': 375.4999694824219, 'w': 82.99998474121094, 'h': 24.999996185302734, 'theta': -0.0}\n",
      "  Object 1: {'xc': 304.0, 'yc': 406.5, 'w': 58.0, 'h': 25.0, 'theta': 180.0}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def visualize_dataset(save_dir='dataset_visualization'):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    st_dataset = SceneTextDataset('train', root_dir='./Q1')\n",
    "    \n",
    "    print(f\"Dataset size: {len(st_dataset)} samples\")\n",
    "    print(f\"Label mapping: {st_dataset.idx2label}\")\n",
    "    \n",
    "    num_samples = min(1, len(st_dataset))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        idx = random.randint(0, len(st_dataset) - 1)\n",
    "        image_tensor, target, image_path = st_dataset[idx]\n",
    "        print(target, image_path)\n",
    "        image = image_tensor.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        for bbox in target['bboxes']:\n",
    "            x1, y1, x2, y2 = bbox.numpy()\n",
    "            plt.gca().add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                fill=False, edgecolor='red', linewidth=2))\n",
    "        \n",
    "        plt.title(f\"Image: {os.path.basename(image_path)}\")\n",
    "        plt.savefig(os.path.join(save_dir, f\"sample_{i}.png\"))\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Sample {i} - {os.path.basename(image_path)}:\")\n",
    "        \n",
    "        json_path = './Q1/annots/' + os.path.basename(image_path) + '.json'\n",
    "        print(json_path)\n",
    "        if os.path.exists(json_path):\n",
    "            with open(json_path, 'r') as f:\n",
    "                import json\n",
    "                data = json.load(f)\n",
    "                print(f\"Number of objects: {len(data['objects'])}\")\n",
    "                \n",
    "                for j, obj in enumerate(data['objects'][:2]):\n",
    "                    print(f\"  Object {j}: {obj['obb']}\")\n",
    "        \n",
    "        print(\"\\n\")\n",
    "\n",
    "visualize_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_sample(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    dataset = SceneTextDataset('test', root_dir=config['dataset_params']['root_dir'])\n",
    "    \n",
    "    if len(dataset) > 0:\n",
    "        image, target, image_path = dataset[0]\n",
    "        return image, target, image_path, config\n",
    "    else:\n",
    "        raise ValueError(\"Dataset is empty. Check dataset path.\")\n",
    "\n",
    "def setup_model(config):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = detection.fasterrcnn_resnet50_fpn(\n",
    "        pretrained=True,  \n",
    "        min_size=config['model_params'].get('min_im_size', 600),\n",
    "        max_size=config['model_params'].get('max_im_size', 1000)\n",
    "    )\n",
    "    \n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(\n",
    "        model.roi_heads.box_predictor.cls_score.in_features,\n",
    "        num_classes=config['dataset_params']['num_classes']\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    return model, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_objectness_maps(model, image, device, output_dir, iteration):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    img_tensor = image.to(device)    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        images = ImageList(img_tensor.unsqueeze(0), [(img_tensor.shape[1], img_tensor.shape[2])])\n",
    "        features = model.backbone(images.tensors)\n",
    "        objectness, _ = model.rpn.head([features[k] for k in features.keys()])\n",
    "    \n",
    "    n_levels = len(objectness)\n",
    "    fig, axes = plt.subplots(1, n_levels, figsize=(n_levels*5, 5))\n",
    "    \n",
    "    if n_levels == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for level, (ax, obj) in enumerate(zip(axes, objectness)):\n",
    "        obj_prob = torch.sigmoid(obj.detach().cpu())\n",
    "        obj_sum = obj_prob[0].sum(dim=0)\n",
    "        obj_norm = (obj_sum - obj_sum.min()) / (obj_sum.max() - obj_sum.min() + 1e-8)\n",
    "        \n",
    "        im = ax.imshow(obj_norm.numpy(), cmap='hot', interpolation='nearest')\n",
    "        ax.set_title(f'Level {level+1}')\n",
    "        fig.colorbar(im, ax=ax)\n",
    "    \n",
    "    fig.suptitle(f'RPN Objectness Maps - Iteration {iteration}')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, f'objectness_iter{iteration}.png'))\n",
    "    plt.close()\n",
    "\n",
    "def visualize_proposals(model, image, image_path, device, output_dir, iteration):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    img_tensor = image.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    images = ImageList(img_tensor.unsqueeze(0), [(img_tensor.shape[1], img_tensor.shape[2])])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = model.backbone(images.tensors)\n",
    "        proposals, _ = model.rpn(images, features)\n",
    "    \n",
    "    original_img = cv2.imread(image_path)\n",
    "    original_rgb = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    proposals_img = original_rgb.copy()\n",
    "    \n",
    "    max_proposals = min(50, len(proposals[0]))\n",
    "    for i in range(max_proposals):\n",
    "        box = proposals[0][i].detach().cpu().numpy().astype(np.int32)\n",
    "        cv2.rectangle(proposals_img, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(proposals_img)\n",
    "    plt.title(f'RPN Proposals (top {max_proposals}) - Iteration {iteration}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, f'proposals_iter{iteration}.png'))\n",
    "    plt.close()\n",
    "\n",
    "def visualize_anchor_assignments(model, image, target, image_path, device, output_dir, iteration):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    img_tensor = image.to(device)\n",
    "    \n",
    "    target_dict = {\n",
    "        'boxes': target['bboxes'].to(device),\n",
    "        'labels': target['labels'].to(device)\n",
    "    }\n",
    "    targets = [target_dict]\n",
    "    \n",
    "    images = [img_tensor]\n",
    "    images, targets = model.transform(images, targets)\n",
    "    features = model.backbone(images.tensors)\n",
    "    \n",
    "    feature_maps = [features[k] for k in features.keys()]\n",
    "    anchors = model.rpn.anchor_generator(images, feature_maps)\n",
    "    labels, matched_gt_boxes = model.rpn.assign_targets_to_anchors(anchors, targets)\n",
    "    \n",
    "    sampled_pos_inds, sampled_neg_inds = model.rpn.fg_bg_sampler(labels)\n",
    "    \n",
    "    pos_anchors = []\n",
    "    neg_anchors = []\n",
    "    \n",
    "    for img_idx, (pos_inds_img, neg_inds_img) in enumerate(zip(sampled_pos_inds, sampled_neg_inds)):\n",
    "        pos_idx = torch.where(pos_inds_img)[0]\n",
    "        neg_idx = torch.where(neg_inds_img)[0]\n",
    "        \n",
    "        pos_idx = pos_idx[:10] if len(pos_idx) > 10 else pos_idx\n",
    "        neg_idx = neg_idx[:10] if len(neg_idx) > 10 else neg_idx\n",
    "        \n",
    "        pos_anchors.append(anchors[img_idx][pos_idx].detach().cpu())\n",
    "        neg_anchors.append(anchors[img_idx][neg_idx].detach().cpu())\n",
    "    \n",
    "    original_img = cv2.imread(image_path)\n",
    "    original_rgb = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    anchors_img = original_rgb.copy()\n",
    "    \n",
    "    for anchor in pos_anchors[0]:\n",
    "        box = anchor.numpy().astype(np.int32)\n",
    "        cv2.rectangle(anchors_img, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "    \n",
    "    for anchor in neg_anchors[0]:\n",
    "        box = anchor.numpy().astype(np.int32)\n",
    "        cv2.rectangle(anchors_img, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 2)\n",
    "    \n",
    "    for box in target['bboxes']:\n",
    "        box = box.numpy().astype(np.int32)\n",
    "        cv2.rectangle(anchors_img, (box[0], box[1]), (box[2], box[3]), (0, 0, 255), 2)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(anchors_img)\n",
    "    plt.title(f'Anchor Assignments - Iteration {iteration}\\nGreen: Positive, Red: Negative, Blue: GT')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, f'anchors_iter{iteration}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_videos(image_dir, output_dir, fps=1):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    viz_types = ['objectness', 'proposals', 'anchors']\n",
    "    \n",
    "    for viz_type in viz_types:\n",
    "        images = sorted([f for f in os.listdir(image_dir) \n",
    "                      if f.startswith(viz_type) and f.endswith('.png')])\n",
    "        \n",
    "        if not images:\n",
    "            continue\n",
    "            \n",
    "        images.sort(key=lambda x: int(x.split('_iter')[1].split('.')[0]))\n",
    "        \n",
    "        first_img = cv2.imread(os.path.join(image_dir, images[0]))\n",
    "        height, width = first_img.shape[:2]\n",
    "        \n",
    "        video_path = os.path.join(output_dir, f'{viz_type}_video.mp4')\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        video = cv2.VideoWriter(video_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        for img_file in images:\n",
    "            img = cv2.imread(os.path.join(image_dir, img_file))\n",
    "            video.write(img)\n",
    "        \n",
    "        video.release()\n",
    "        print(f\"Created video: {video_path}\")\n",
    "\n",
    "def simulate_training_steps(model, image, target, image_path, device, output_dir, num_steps=5):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    objectness_dir = os.path.join(output_dir, 'objectness')\n",
    "    proposals_dir = os.path.join(output_dir, 'proposals')\n",
    "    anchors_dir = os.path.join(output_dir, 'anchors')\n",
    "    os.makedirs(objectness_dir, exist_ok=True)\n",
    "    os.makedirs(proposals_dir, exist_ok=True)\n",
    "    os.makedirs(anchors_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"Visualizing initial state...\")\n",
    "    visualize_objectness_maps(model, image, device, objectness_dir, 0)\n",
    "    visualize_proposals(model, image, image_path, device, proposals_dir, 0)\n",
    "    visualize_anchor_assignments(model, image, target, image_path, device, anchors_dir, 0)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=0.005,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0005\n",
    "    )\n",
    "    \n",
    "    model.train()\n",
    "    for step in range(1, num_steps + 1):\n",
    "        print(f\"Simulating training step {step}...\")\n",
    "        images = [image.to(device)]\n",
    "        targets_list = [{\n",
    "            'boxes': target['bboxes'].to(device),\n",
    "            'labels': target['labels'].to(device)\n",
    "        }]\n",
    "        \n",
    "        loss_dict = model(images, targets_list)\n",
    "        \n",
    "        if isinstance(loss_dict, dict):\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "        elif isinstance(loss_dict, list):\n",
    "            print(\"Warning: Model returned detections instead of losses. Setting model to train mode.\")\n",
    "            model.train()\n",
    "            loss_dict = model(images, targets_list)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "        else:\n",
    "            print(\"Warning: Unexpected loss format. Computing losses manually.\")\n",
    "            images_t, targets_t = model.transform(images, targets_list)\n",
    "            \n",
    "            features = model.backbone(images_t.tensors)\n",
    "            \n",
    "            proposals, rpn_losses = model.rpn(images_t, features, targets_t)\n",
    "            \n",
    "            detections, detector_losses = model.roi_heads(\n",
    "                features, proposals, images_t.image_sizes, targets_t)\n",
    "            \n",
    "            losses = sum(loss for loss in rpn_losses.values())\n",
    "            losses += sum(loss for loss in detector_losses.values())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        visualize_objectness_maps(model, image, device, objectness_dir, step)\n",
    "        visualize_proposals(model, image, image_path, device, proposals_dir, step)\n",
    "        visualize_anchor_assignments(model, image, target, image_path, device, anchors_dir, step)\n",
    "    \n",
    "    print(\"Creating videos...\")\n",
    "    create_videos(objectness_dir, output_dir)\n",
    "    create_videos(proposals_dir, output_dir)\n",
    "    create_videos(anchors_dir, output_dir)\n",
    "    \n",
    "    print(f\"Visualization completed. Results saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'background', 1: 'text'}\n",
      "Loaded image: ./Q1/img/img47.jpg\n",
      "Model created on device: cuda\n",
      "Visualizing initial state...\n",
      "Simulating training step 1...\n",
      "Simulating training step 2...\n",
      "Warning: Model returned detections instead of losses. Setting model to train mode.\n",
      "Simulating training step 3...\n",
      "Warning: Model returned detections instead of losses. Setting model to train mode.\n",
      "Simulating training step 4...\n",
      "Warning: Model returned detections instead of losses. Setting model to train mode.\n",
      "Simulating training step 5...\n",
      "Warning: Model returned detections instead of losses. Setting model to train mode.\n",
      "Creating videos...\n",
      "Created video: rpn_visualization_results/objectness_video.mp4\n",
      "Created video: rpn_visualization_results/proposals_video.mp4\n",
      "Created video: rpn_visualization_results/anchors_video.mp4\n",
      "Visualization completed. Results saved to rpn_visualization_results\n"
     ]
    }
   ],
   "source": [
    "config_path = 'config/st.yaml'\n",
    "output_dir = 'rpn_visualization_results'\n",
    "try:\n",
    "    image, target, image_path, config = load_dataset_sample(config_path)\n",
    "    print(f\"Loaded image: {image_path}\")\n",
    "    \n",
    "    model, device = setup_model(config)\n",
    "    print(f\"Model created on device: {device}\")\n",
    "    \n",
    "    simulate_training_steps(model, image, target, image_path, device, output_dir)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
